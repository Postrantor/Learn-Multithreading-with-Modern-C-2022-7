WEBVTT

0
00:00.660 --> 00:06.240
Hello there. In this video, we're going to look at the motivation for concurrency. Or, in other words,

1
00:06.240 --> 00:08.550
why is concurrency considered important?

2
00:09.990 --> 00:17.520
Concurrency first appeared on mainframes in the 1960s, but it was not until the 1990s before the world

3
00:17.520 --> 00:23.010
of workstations and personal computers began to regard it as an important technology.

4
00:24.030 --> 00:27.250
And there were four main reasons why that happened at that particular time.

5
00:27.540 --> 00:32.870
These were all developments in the computer world, which were more or less happening at the same time.

6
00:34.770 --> 00:37.080
The first one was the rise of the Internet.

7
00:37.860 --> 00:41.460
Previously, a server might handle, maybe a few hundred clients

8
00:41.460 --> 00:48.600
if it was really busy. Now, you could have a server that might have 100,000 or maybe even a million simultaneous

9
00:48.600 --> 00:56.490
clients or more. Clearly, the amount of work that a server could do at any one time, its throughput, needed

10
00:56.490 --> 01:02.070
to increase dramatically and threading was one of the techniques that was used to deliver that.

11
01:04.250 --> 01:11.240
The second one was the rise of Windows. Users on personal computers could now access several running

12
01:11.240 --> 01:16.220
programs at the same time, and they expected to be able to interact with them even if the programs

13
01:16.220 --> 01:17.660
were busy doing something else.

14
01:18.530 --> 01:24.530
So this means that Windows programs had to be able to do more than one thing at a time, and that means

15
01:24.530 --> 01:28.000
multi threading. The third one.

16
01:28.010 --> 01:30.340
was the rise of games and multimedia.

17
01:30.740 --> 01:33.890
These typically involve lots of mathematical computations.

18
01:34.940 --> 01:38.490
If the frame rate is too low, the game will not play smoothly.

19
01:38.870 --> 01:43.970
The video will look jerky, or have bits missing, and the audio will have clicks and pops.

20
01:45.890 --> 01:49.260
And the final event was the rise of multicore processors.

21
01:50.210 --> 01:56.060
We now had computers which had several processors, and we wanted programs which could take full

22
01:56.060 --> 01:56.960
advantage of this.

23
02:00.080 --> 02:05.960
So let's look at each of these in a bit more detail. Traditionally with servers, you would have everything

24
02:05.960 --> 02:07.630
done as a single process.

25
02:08.210 --> 02:14.270
So there would be a single process, which was the server, and there would be other processes which are

26
02:14.660 --> 02:15.830
child processes.

27
02:16.490 --> 02:22.840
Every time the server received a client connection, it would start up a new process, a new child process,

28
02:22.840 --> 02:29.570
and that child process would deal with the clients while the server carried on and waited

29
02:29.570 --> 02:30.500
for the next connection.

30
02:32.060 --> 02:37.090
These processes are all separate running applications with their own address space.

31
02:37.730 --> 02:40.540
There is no direct communication between them.

32
02:41.360 --> 02:47.030
If you want the server to be able to send some data to a client, you would need to put in a communication

33
02:47.030 --> 02:47.410
channel.

34
02:49.040 --> 02:54.860
These interprocess communication channels have overhead and there is also a certain amount of overhead

35
02:55.160 --> 02:56.570
from creating a process.

36
02:57.020 --> 03:03.230
It's not too bad on Unix systems, but on Windows systems, it's slow and resource intensive.

37
03:03.920 --> 03:10.850
So all this process creation and adding interprocess communication will increase overhead, and that will

38
03:10.850 --> 03:14.330
reduce the scalability and the amount of work that servers can do.

39
03:17.550 --> 03:24.480
Separation of concerns. Let's imagine you are editing a large document and you are using a single threaded program.

40
03:25.590 --> 03:28.990
You start some action which is going to take a long time to complete.

41
03:29.400 --> 03:32.100
You could be compiling an index for the document.

42
03:32.550 --> 03:39.120
You could be calculating where the page numbers go, or you could even - and we are still in the 1990s, remember -

43
03:39.630 --> 03:41.580
you could even be saving to a floppy disk!

44
03:42.330 --> 03:44.610
Clunk, clunk, clunk-clunk!

45
03:46.860 --> 03:47.810
While that's happening,

46
03:47.820 --> 03:51.590
The user interface of the program will be completely unresponsive.

47
03:51.930 --> 03:55.470
You can click on it with your mouse, you can press keys, you can do everything,

48
03:55.470 --> 04:01.800
and it'll completely ignore you. If you switch to another application which covers up the program

49
04:02.400 --> 04:07.050
and then eventually you come back to the program, the window will have gone.

50
04:07.060 --> 04:10.220
You just get a grey box, possibly with an hourglass.

51
04:11.460 --> 04:16.890
And the reason for that is that the program can only do one thing at a time, which is performing the

52
04:16.890 --> 04:18.960
action that you asked it to perform.

53
04:20.760 --> 04:26.470
It cannot respond to GUI events, such as being told "someone has clicked a mouse" or "someone has pressed this key",

54
04:26.760 --> 04:30.240
"you've got to redraw yourself", because it is busy performing the action.

55
04:30.540 --> 04:32.170
All these events are stuck in a queue.

56
04:33.870 --> 04:35.970
Then finally, the action will end.

57
04:36.360 --> 04:40.470
The program will read all the events off the queue and process them instantly.

58
04:40.830 --> 04:42.570
Then all sorts of weird things will happen,

59
04:42.570 --> 04:48.210
depending on what you did. You might find out that you have erased your document or overwritten it with

60
04:48.210 --> 04:51.780
a load of garbage and saved it. Generally not desirable.

61
04:54.650 --> 05:00.080
With fast numerical computation, typically, if you were worried about this, you were a scientist

62
05:00.080 --> 05:07.370
or similar, you worked at a university or a research institution and you had persuaded your employers

63
05:07.580 --> 05:13.040
to buy you and your colleagues a nice new toy, which has lots of processors which are specially wired

64
05:13.040 --> 05:16.360
up so that they could coordinate with each other.

65
05:17.870 --> 05:23.570
You wrote a program using a special language, maybe a parallel version of Fortran or Occam, and

66
05:23.570 --> 05:28.220
then this would divide up your program and run different parts of it on different processors at the same

67
05:28.220 --> 05:28.560
time.

68
05:28.880 --> 05:31.060
So that would achieve parallel processing.

69
05:31.880 --> 05:35.480
So instead of taking three months, your simulation might take three weeks.

70
05:37.280 --> 05:40.360
Nowadays we have hardware which supports parallel programming.

71
05:40.640 --> 05:44.500
You can buy a normal computer, or even a phone, which will do that.

72
05:45.300 --> 05:46.880
So it doesn't cost very much.

73
05:47.540 --> 05:52.010
It's supported by standard programming languages which have now got features for parallel processing

74
05:52.160 --> 05:54.480
C++ has some, which we will look at later.

75
05:55.340 --> 05:58.760
And finally, you do not have to fight with your colleagues to actually get to run your program.

76
06:01.690 --> 06:08.560
Effective use of hardware. By the time we got to the 1990s, designers at companies like Intel and

77
06:08.560 --> 06:14.260
AMD had spent a couple of decades making chips that were bigger and bigger and more and more complicated

78
06:14.620 --> 06:17.080
and could do more and more things faster and faster.

79
06:18.250 --> 06:23.500
They were now starting to be affected by the limits imposed by the laws of physics. In particular,

80
06:23.500 --> 06:26.650
the one which says nothing can travel faster than the speed of light.

81
06:28.060 --> 06:33.400
In practice, electrons can move through silicon at maybe five or 10 percent of the speed of light.

82
06:34.360 --> 06:39.010
If your processor cycle time is one nanosecond, that might mean

83
06:40.240 --> 06:47.130
a quarter of an inch, half a centimetre, which is a very long way, actually, but if your chip is three

84
06:47.130 --> 06:51.930
inches or five centimetres across, it's going to take a long time just for an electron to go from one

85
06:51.930 --> 06:53.960
side of the processor chip to the other.

86
06:55.020 --> 06:59.940
And that's going to slow down the rate at which data can move around and instructions can be processed.

87
07:01.440 --> 07:03.420
The other problem is the heat generated.

88
07:03.870 --> 07:10.110
While the electrons move through silicon, they generate heat and the amount of heat was getting bigger

89
07:10.110 --> 07:10.720
and bigger.

90
07:12.060 --> 07:17.880
If you extrapolate it, you eventually get to things like the temperature of molten steel or the surface

91
07:17.880 --> 07:18.410
of the sun.

92
07:18.840 --> 07:22.080
And that's really not something that you want to have sitting on your desk!

93
07:23.730 --> 07:29.940
So they abandoned the idea of having one mega processor and instead they started designing chips

94
07:29.940 --> 07:32.190
which had several smaller processor cores.

95
07:34.240 --> 07:40.240
If you have a single threaded program, it can only run on one core, or maybe one core at a time, if it

96
07:40.240 --> 07:47.920
gets switched around between cores. If you have a computer with 16 or eight or four cores, then you are

97
07:47.920 --> 07:51.190
obviously not making the full use of the system potential.

98
07:51.610 --> 07:57.250
If you have a computer which only runs one program and it only uses one core, then you are only using

99
07:57.250 --> 07:59.220
a small fraction of the system's power.

100
08:02.680 --> 08:08.020
So concurrency gives our programs the ability to do more than one thing at the same time. In the case

101
08:08.020 --> 08:12.660
of servers, we can have a child which is running as a thread inside the server process.

102
08:13.150 --> 08:17.070
So we don't need to create a new process every time we get a new connection.

103
08:18.080 --> 08:23.120
And because they are all part of the same process, they have the same address space so they can directly

104
08:23.120 --> 08:25.560
access data if they choose to share it.

105
08:26.570 --> 08:31.280
So that removes all the overhead connected with interprocess communication and process creation.

106
08:31.500 --> 08:36.420
It does add overhead from thread creation and safely sharing data.

107
08:36.770 --> 08:38.230
But that's a different question.

108
08:39.490 --> 08:45.750
For separation of concerns, we can write a program which is able to respond to GUI events promptly

109
08:46.120 --> 08:51.460
while it continues to perform a long running task, and we can do that by providing separate threads

110
08:51.640 --> 08:52.780
for the GUI and the task.

111
08:53.940 --> 08:59.280
With numerical computation and using the hardware, we can write programs which can execute on different

112
08:59.280 --> 09:00.460
cores at the same time.

113
09:02.100 --> 09:04.510
So concurrency addresses all these problems.

114
09:04.680 --> 09:10.110
It doesn't completely solve things because it adds some problems of its own, but it is progress.

115
09:12.690 --> 09:18.150
And finally, just one thing to clear up, there is a small but important difference between concurrency

116
09:18.150 --> 09:19.230
and parallelism.

117
09:20.490 --> 09:25.050
Concurrency means we have different tasks which are running at the same time.

118
09:25.410 --> 09:28.810
So these are things which represent different concepts in the program.

119
09:29.370 --> 09:33.680
For example, if we have a server and a child, those are different concepts.

120
09:34.230 --> 09:35.700
So that would be concurrency.

121
09:36.540 --> 09:43.100
Processing GUI events and calculating an index or page numbers are conceptually different tasks.

122
09:43.410 --> 09:45.690
So if they run at the same time, they are concurrent.

123
09:47.220 --> 09:50.450
If the tasks are the same, then that is parallelism.

124
09:50.760 --> 09:54.240
So parallelism means that we have the same tasks running at the same time.

125
09:54.660 --> 09:59.560
If we take a numerical computation and run different parts of it on different cores at the same time.

126
09:59.880 --> 10:05.070
So we are doing the same addition or multiplication or whatever on different cores at the same time.

127
10:05.520 --> 10:07.500
That would be an example of parallelism.

128
10:08.620 --> 10:09.710
Okay, that's it.

129
10:10.350 --> 10:11.370
I'll see you next time.