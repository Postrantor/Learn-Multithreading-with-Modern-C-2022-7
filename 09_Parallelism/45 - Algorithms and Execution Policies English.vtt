WEBVTT

0
00:00.120 --> 00:00.750
Hello again!

1
00:01.110 --> 00:05.160
In this video, we are going to talk about algorithms and execution policies.

2
00:07.000 --> 00:14.080
When execution policies came along in C++17, most of the algorithms in the standard library were

3
00:14.080 --> 00:17.110
re-specified to support execution policies.

4
00:17.920 --> 00:24.220
There were a few exceptions, which are algorithms which can really be only done sequentially. For example,

5
00:24.220 --> 00:25.840
equal_range and iota.

6
00:28.110 --> 00:34.050
There are some algorithms in <numeric> which are specified as being sequential-only.
They came up with

7
00:34.050 --> 00:37.710
some new functions with different names, which support policies.

8
00:38.490 --> 00:41.480
So there is a new version of accumulate called reduce.

9
00:42.210 --> 00:47.130
And there are two new versions of partial_sum which are called inclusive and exclusive scan.

10
00:48.260 --> 00:56.800
There's also a new "fused" algorithm which combines two separate algorithms: transform_reduce combines

11
00:56.810 --> 01:02.540
transform and inner_product.
And we will be looking at these new functions later on in this section.

12
01:05.680 --> 01:12.210
We need to talk about how exceptions are handled with algorithms. With the original non-policy algorithms,

13
01:12.220 --> 01:13.210
it is very straightforward.

14
01:13.210 --> 01:16.780
It is just like any other code. You have a try/catch block.

15
01:17.920 --> 01:20.560
So you put the algorithm call inside the try block.

16
01:21.070 --> 01:28.270
And if there is something that might throw, a lambda expression or maybe a functor. If there is an exception,

17
01:28.570 --> 01:31.480
then it gets caught in the catch block and you can handle it there.

18
01:33.430 --> 01:38.320
When you have an execution policy, that does not work because there are probably going to be multiple

19
01:38.320 --> 01:43.840
threads executing in parallel. Each thread will have its own stack and you cannot transfer an exception

20
01:44.380 --> 01:45.730
from one thread to another.

21
01:48.820 --> 01:55.210
So the result is rather drastic. If an uncaught exception is thrown, the terminate function is called,

22
01:55.450 --> 01:57.420
and by default that will end the program.

23
01:58.990 --> 02:04.240
If we imagine we have an algorithm call, which is going to apply a function to every element in the

24
02:04.240 --> 02:10.870
container. If this function throws an exception and handles the exception internally, so it does not escape

25
02:10.870 --> 02:14.740
outside the function call, then the execution of the algorithm is going to continue.

26
02:16.130 --> 02:22.510
If the applied function does not handle the exception itself or if it rethrows it, then it goes outside

27
02:22.550 --> 02:28.010
the function call into the algorithm and then the algorithm will terminate.

28
02:29.860 --> 02:34.690
It's also possible for the algorithm call itself to throw an exception, even if it does not call any

29
02:34.690 --> 02:35.380
other functions.

30
02:36.640 --> 02:41.890
If it needs some temporary memory resources, for example, to manage the new threads that it is creating

31
02:42.340 --> 02:46.120
and it cannot obtain these, it is going to throw the standard bad_alloc exception.

32
02:49.890 --> 02:57.600
Let's look at an example. We have got a vector of ints.
We are going to call the sort of algorithm.

33
02:57.900 --> 03:01.430
We are going to use the entire range of elements in the vector.

34
03:02.520 --> 03:04.510
So that is the data that is going to be sorted.

35
03:05.250 --> 03:10.710
We are going to compare the elements by using a lambda expression, and this lambda expression is going

36
03:10.710 --> 03:11.850
to throw an exception.

37
03:14.200 --> 03:20.650
We are calling the sort algorithm inside a try blog, so we would expect that the exception would be caught

38
03:20.950 --> 03:22.570
and handled in the catch block.

39
03:23.470 --> 03:24.610
So let's see what happens.

40
03:25.930 --> 03:29.170
So as we would expect, the exception is thrown.

41
03:29.470 --> 03:31.020
It is caught in the catch block.

42
03:31.210 --> 03:36.940
And that is exactly as we would expect. If we now add an execution policy...

43
03:40.580 --> 03:46.370
So let's use sequential for simplicity and then let's see what happens.

44
03:47.030 --> 03:49.040
So the program has terminated abnormally.

45
03:55.140 --> 04:00.870
The reason for this is that an algorithm which uses an execution policy can start up multiple threads.

46
04:01.920 --> 04:08.880
In C++, each thread has its own execution stack, and it is quite difficult to communicate exceptions

47
04:08.880 --> 04:09.620
between threads.

48
04:10.500 --> 04:16.380
There is the futures mechanism, but that would add overhead and it would defeat the object of using

49
04:16.650 --> 04:17.910
parallel execution.

50
04:18.870 --> 04:21.420
So instead, the program just terminates.

51
04:25.080 --> 04:29.460
Now, you may be thinking that all this execution policy stuff sounds very exciting,
and you are going

52
04:29.460 --> 04:32.610
to rush off and convert all your algorithm calls to use them.

53
04:33.000 --> 04:35.850
But before you do that, there are some downsides that you need to think about.

54
04:36.830 --> 04:41.030
The first one is that an execution policy may not actually do anything.

55
04:42.470 --> 04:48.890
The reason for this is that compiler writers are still in the process of implementing execution policies

56
04:48.890 --> 04:49.790
for algorithms.

57
04:51.770 --> 04:59.090
The C++ language does give them plenty of freedom in this respect.
If they have not yet found an effective

58
04:59.630 --> 05:04.900
parallel or vectorized algorithm,
they are allowed to supply a sequential implementation.

59
05:05.510 --> 05:08.990
So it may be that you ask for parallel and vectorized and you still get sequential.

60
05:10.930 --> 05:15.850
There is extra overhead involved, especially for the ones which do parallel processing.
The algorithm

61
05:15.850 --> 05:20.200
will have to create threads and manage them,
and that all uses up CPU time.

62
05:22.220 --> 05:27.440
There are things called complexity requirements which basically say how fast an algorithm has to run

63
05:27.650 --> 05:29.260
for a given number of elements.

64
05:30.760 --> 05:36.340
The old algorithms have complexity requirements, for example, if you're doing a sort, the number

65
05:36.340 --> 05:40.330
of swap operations must be equal to or lower than the number of elements.

66
05:42.110 --> 05:47.960
When you move on to parallel algorithms, you often find that algorithms that work well in parallel

67
05:48.170 --> 05:51.590
have more operations than ones which work well sequentially.

68
05:52.700 --> 05:55.250
So they actually perform more operations.

69
05:55.790 --> 05:59.060
But because they run in parallel, they take less time overall.

70
06:00.550 --> 06:05.200
So this means that the parallel algorithms may have weaker complexity requirements than the original

71
06:05.200 --> 06:11.260
ones or even no requirements at all. Obviously compiler writers want to have the most efficient

72
06:12.040 --> 06:17.530
implementations possible, for benchmarks. But it may be they have not found a good one yet,
so they

73
06:17.530 --> 06:18.870
can put in a suboptimal one.

74
06:19.450 --> 06:20.440
So that might be a problem.

75
06:20.830 --> 06:24.490
Some of these problems are going to go away, of course, once there is better compiler support.

76
06:28.170 --> 06:30.300
So when should we use an execution policy?

77
06:31.020 --> 06:35.130
The best advice is not to use one at all unless you know that you need one.

78
06:36.380 --> 06:41.480
There's a famous saying, which I think every programmer we should have stuck on their wall, which is "premature

79
06:41.480 --> 06:43.720
optimization is the root of all evil".

80
06:45.320 --> 06:50.090
The reasoning behind that is when you try to optimize, you often create complicated code, which is

81
06:50.090 --> 06:52.100
hard to understand and more difficult to get

82
06:52.100 --> 06:52.470
right.

83
06:53.880 --> 06:59.280
What you should do is to get clear, understandable, well-structured code that produces the correct

84
06:59.280 --> 07:01.070
answers,
and then try to speed it up.

85
07:03.090 --> 07:09.420
Some situations where you should definitely not use an execution policy: If you have a task which is

86
07:09.630 --> 07:11.100
best done sequentially.

87
07:11.310 --> 07:18.290
For example, incrementing the value of each element in turn.
If the order of operations is important.

88
07:19.050 --> 07:23.660
if you try to re-order subtractions, then you will get some very wrong results.

89
07:25.330 --> 07:30.570
If the algorithm is going to call a function which might throw an exception,
and immediately terminating

90
07:30.580 --> 07:32.040
the program is not acceptable.

91
07:34.550 --> 07:40.910
If the code involves data races, then parallel execution means that you will need to protect against a

92
07:40.910 --> 07:47.510
data race, and the cost of locking a mutex or using an atomic variable is probably going to be much greater

93
07:47.510 --> 07:49.550
than the cost of not having an execution policy.

94
07:51.290 --> 07:56.690
On the other hand, you should consider using an execution policy if measuring the performance of your

95
07:56.690 --> 08:02.270
program shows a definite and worthwhile improvement in performance. And of course, providing you can

96
08:02.270 --> 08:03.580
do it safely and correctly.

97
08:07.160 --> 08:12.470
So which execution policy should you use? Sequential execution.

98
08:12.500 --> 08:16.030
may sound a bit pointless, but in fact it is useful for debugging.

99
08:16.820 --> 08:22.520
The reason for that is that sequential execution is much easier to understand than parallel or vectorized

100
08:22.520 --> 08:23.150
execution.

101
08:23.430 --> 08:25.910
So it is much easier to spot when something is going wrong.

102
08:27.720 --> 08:33.480
This is almost the same as the non policy version, but it allows out of order execution and

103
08:33.480 --> 08:36.140
if any exceptions are thrown, it will terminate immediately.

104
08:38.890 --> 08:44.620
The obvious one to use is parallel unsequenced execution, which is both parallel and vector instructions.

105
08:45.190 --> 08:50.080
So that should give the biggest improvement in performance, but it also has the strictest requirements.

106
08:50.560 --> 08:55.930
You need to make sure there cannot be any data races and you need to make sure that the code does not modify

107
08:55.930 --> 08:56.650
shared state.

108
08:57.730 --> 09:04.390
So you need to avoid allocating and deallocating memory, and you also need to avoid mutexes, which

109
09:04.390 --> 09:06.380
rather conflicts with the data race [avoidance].

110
09:07.210 --> 09:09.430
So this is best really for pure numerical work.

111
09:10.780 --> 09:13.910
Parallel execution can be used when vectorization is not safe.

112
09:14.260 --> 09:18.400
So if you are going to be locking mutexes or operating on memory.

113
09:19.750 --> 09:25.870
Unsequenced execution in C++20. That is just vectorization on its own.
You can even use that in a

114
09:25.870 --> 09:26.910
single threaded program.

115
09:28.280 --> 09:31.240
The only requirement is that the code doesn't modify shared state.

116
09:31.880 --> 09:33.680
OK, so that's it for this video.

117
09:33.740 --> 09:34.620
I'll see you next time.

118
09:34.640 --> 09:36.710
But meanwhile, keep coding!