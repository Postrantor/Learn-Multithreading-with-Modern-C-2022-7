WEBVTT

0
00:00.180 --> 00:00.750
Hello again!

1
00:00.960 --> 00:06.690
In this video, we are going to continue looking at the new parallel algorithms in C++17.

2
00:08.790 --> 00:12.930
First, we are going to take a slight detour and look at the transform algorithm.

3
00:13.710 --> 00:17.280
This takes a range of elements and a callable object.

4
00:17.970 --> 00:24.780
It will call that object on every element in the iterator range, 
and the result of each call will

5
00:24.780 --> 00:30.130
be used to populate a destination container.
In functional languages,

6
00:30.150 --> 00:35.220
there is a similar concept called map. That name is used for something else in C++.

7
00:37.450 --> 00:44.320
So here is an example of transform. We have a vector, which is going to be the input
and a vector v2

8
00:44.320 --> 00:46.180
which is going to be the output.

9
00:49.720 --> 00:52.910
Our callable object will double its argument.

10
00:53.170 --> 01:00.010
So this is going to be called on every element in v
and the result is going to be stored as the elements

11
01:00.010 --> 01:00.820
of v2.

12
01:01.030 --> 01:08.820
So we would expect that the elements of v2 are two, four, six and eight from doubling these.
And there

13
01:08.890 --> 01:10.450
are, two four, six, eight.

14
01:15.330 --> 01:19.770
So that allows us to apply a unary operator to transform a vector.

15
01:20.430 --> 01:26.460
There's also an overload of transform, which allows us to apply a binary operator
to transform

16
01:26.760 --> 01:29.100
pairs of elements from different containers.

17
01:29.640 --> 01:33.870
So this version will take elements from two containers and combine them.

18
01:36.430 --> 01:42.910
So in this case, we have two input containers. v3 is going to be the output container.

19
01:45.260 --> 01:51.680
The callable object is going to add its arguments, so this is going to be called on every element

20
01:51.680 --> 01:53.870
in v and v2.
And it is going to add them.

21
01:53.880 --> 02:01.070
So one plus five, and then that will store six as the first elements of v3.
Two plus six will give eight as the

22
02:01.070 --> 02:02.680
second element and so on.

23
02:04.550 --> 02:05.210
So there we are.

24
02:05.570 --> 02:10.610
The elements in this vector are the sum of the corresponding elements in the input vectors.

25
02:15.260 --> 02:20.210
We have touched upon this already, but one of the most common ways of doing parallel programming is to

26
02:20.210 --> 02:27.290
split the problem up into small units, have one thread or processor which handles each sub-unit,
and then

27
02:27.530 --> 02:34.460
you combine all the results from each sub-unit to get the complete result.
In C++,

28
02:34.470 --> 02:37.160
you can do this by dividing the data into subsets.

29
02:38.600 --> 02:44.030
You start a number of threads, each one of which will call transform and perform some sort of operation

30
02:44.030 --> 02:50.120
on part of the data,
and then you can call reduce to combine together the results from each thread

31
02:50.360 --> 02:51.410
into the final answer.

32
02:52.670 --> 02:55.010
The problem is that these are two separate functions.

33
02:56.030 --> 03:00.020
So each thread that calls transform has to write data to a container.

34
03:01.490 --> 03:04.370
The reduce process has to read all these containers.

35
03:05.550 --> 03:10.410
The reduce function cannot be called until all the transform threads have finished.

36
03:11.650 --> 03:16.990
Because all the transform threads will be taken down and then reduce will have to start up some threads

37
03:16.990 --> 03:20.740
as well, and it would be much more efficient if we could just re-use these threads.

38
03:22.980 --> 03:28.140
Another way of thinking about this is going back to the early days of programming,
before there were

39
03:28.140 --> 03:34.190
things like interprocess communication. If you wanted programs to share data,
you would have to get one program

40
03:34.200 --> 03:39.600
to write its data to file,
and then the next program would have to read its data from that file or

41
03:40.050 --> 03:41.280
perhaps tape in those days.

42
03:42.660 --> 03:44.730
The problem is that writing to file is very slow.

43
03:45.800 --> 03:51.080
What they did was invent various forms of interprocess communication in which the data is held in memory,

44
03:51.800 --> 03:55.880
so that avoids all the output of writing to file and reading back in again.

45
03:56.900 --> 03:59.970
It also introduces the possibility of streaming data.

46
04:00.410 --> 04:05.640
So the second program can start reading data as soon as the first program produces it.

47
04:09.740 --> 04:17.420
C++ provides a combined transform_reduce function to remove this overhead.
It is actually based on

48
04:17.420 --> 04:19.770
an existing algorithm called inner_product.

49
04:20.240 --> 04:21.680
So let's find out what that does.

50
04:24.590 --> 04:31.340
inner_product is actually designed for numerical processing. 
It takes two containers, multiplies the

51
04:31.340 --> 04:35.150
corresponding elements together, and then calculates the sum of all these products.

52
04:38.410 --> 04:45.610
So as an example, we have two vectors x and y. We call inner_product.
We give the iterator range,

53
04:45.610 --> 04:54.220
which is all the elements in x, we give the start of y and the initial value of the sum,
and then

54
04:54.250 --> 04:55.380
this will multiply the values.

55
04:55.390 --> 04:59.290
So one times five, two times four, three times three and so on.

56
04:59.440 --> 05:03.700
And it is going to add all those together.
And the result is 35.

57
05:10.110 --> 05:15.150
transform_reduce works exactly the same way,
except we have the option of specifying an execution

58
05:15.150 --> 05:15.610
policy.

59
05:16.800 --> 05:17.490
So here we are.

60
05:17.500 --> 05:24.720
Let's see, we get the same result, 35. Basically the same code.
We just replace inner_product by transform_

61
05:24.720 --> 05:25.230
reduce.

62
05:29.960 --> 05:34.490
You may say "that is not very interesting, I do not do very much matrix multiplication in my work."

63
05:35.240 --> 05:38.030
The interesting thing is that you can overload the operators.

64
05:39.450 --> 05:42.490
We can replace the multiplication by a "transform" function.

65
05:42.840 --> 05:48.660
So this takes two arguments of the type of the elements of the containers,
and it does something and

66
05:48.660 --> 05:55.650
it returns a value which has some return type,
and then we can replace the addition operator by a "reduce"

67
05:55.650 --> 05:56.070
function.

68
05:56.730 --> 05:58.020
This will take two arguments of

69
05:58.020 --> 06:04.680
the returned type, do something with them and return another value of the result type.
So we can

70
06:04.860 --> 06:10.620
do some kind of parallel operation here and then we can combine together the results here.

71
06:14.260 --> 06:19.050
An example which is not really going too far away from maths. It is a scientific experiment,
we have done

72
06:19.050 --> 06:19.670
an experiment.

73
06:19.680 --> 06:21.600
We have got a vector here which contains our results.

74
06:22.530 --> 06:24.030
We have also got another vector.

75
06:24.270 --> 06:27.570
We have calculated what the results should be according to the theory.

76
06:30.140 --> 06:35.330
So we want to find out which one of these data points has the biggest erorr,
which means the biggest

77
06:35.330 --> 06:41.720
discrepancy between theory and practice.
We could do this using a transform_reduce.

78
06:42.980 --> 06:47.330
We replace the multiplication by some kind of transformation operation.

79
06:47.870 --> 06:50.080
So this is the stage that is done in parallel.

80
06:50.480 --> 06:55.760
In this case, we are finding the differences between the corresponding elements in the two vectors.

81
06:57.260 --> 07:00.830
Then we replace the addition by a reduce operation.

82
07:02.030 --> 07:05.890
So this is going to go through the differences and produce the final answer.

83
07:06.140 --> 07:09.410
In this case, it is going to find the largest one out of those differences.

84
07:12.120 --> 07:17.730
So it is going to work like this. We will replace the multiplication by a function that finds the difference

85
07:17.730 --> 07:23.340
between corresponding elements, instead of multiplying them together.
And instead of adding together

86
07:23.340 --> 07:27.330
the results, we are going to find the result which has the biggest value.

87
07:29.790 --> 07:35.940
So here we are, the transform stage is where we find the difference between the two elements and

88
07:35.940 --> 07:39.510
then the reduce stage is where we find the biggest difference.

89
07:44.030 --> 07:50.750
So here is an example. I have created a vector which has the results which are predicted by theory,
and

90
07:50.750 --> 07:53.690
then some results which might have come from actually doing the experiment.

91
07:55.310 --> 07:57.050
Then we call transform_reduce.

92
07:57.650 --> 08:01.010
We find the absolute difference between each element.

93
08:01.040 --> 08:06.440
We have to use the floating point version because these are doubles.
And then we go through and find

94
08:06.440 --> 08:06.950
the maximum.

95
08:07.520 --> 08:09.060
So let's see what that gives us.

96
08:10.280 --> 08:10.720
There we are.

97
08:10.730 --> 08:13.250
The maximum difference is nought point nought three.

98
08:15.600 --> 08:18.450
And I think that is those elements there.

99
08:25.860 --> 08:28.140
And there we again, nought point nought three.

100
08:28.530 --> 08:30.210
OK, so that's it for this video.

101
08:30.390 --> 08:31.280
I'll see you next time.

102
08:31.330 --> 08:33.390
But meanwhile, keep coding!